# ğŸ¯ FLOW TRAINING - NUMPUZ AI (3x3 â†’ 15x15)

## ğŸ“Œ Tá»•ng quan kiáº¿n trÃºc

Má»—i Ä‘á»™ khÃ³ game NxN Ä‘Æ°á»£c xá»­ lÃ½ trong má»™t **phrase** riÃªng.  
Má»—i phrase lÃ  file Python Ä‘á»™c láº­p `phrase<number>_nxn.py` â€” tá»± Ä‘á»™ng táº¡o dataset, huáº¥n luyá»‡n, xuáº¥t artifacts.  

**Luá»“ng I/O giá»¯a cÃ¡c phrase:**
- Phrase nháº­n model (weights + config) tá»« phrase trÆ°á»›c Ä‘á»ƒ transfer learning.
- Phrase sinh ra: dataset, model, metrics, logs, vÃ  áº£nh trá»±c quan.
- Phrase tiáº¿p theo dÃ¹ng model Ä‘áº§u ra lÃ m input.

---

## ğŸ® PHRASE 1: FOUNDATION (3x3) â€” `phrase1_3x3.py`

### ğŸ”¹ Input
- KhÃ´ng cÃ³ input tá»« phrase trÆ°á»›c (train tá»« Ä‘áº§u).

### ğŸ”¹ Thuáº­t toÃ¡n (Dataset generation & Supervised training)
1. **Sinh puzzle (50k):**
   - Báº¯t Ä‘áº§u tá»« tráº¡ng thÃ¡i solved.
   - Random shuffle báº±ng chuá»—i há»£p lá»‡ vá»›i sá»‘ bÆ°á»›c `k âˆˆ [5,20]`.
   - Kiá»ƒm tra solvability báº±ng inversion count.
2. **Ground-truth path:**
   - DÃ¹ng **A\*** vá»›i heuristic **Manhattan distance** Ä‘á»ƒ tÃ¬m Ä‘Æ°á»ng giáº£i tá»‘i Æ°u cho má»—i puzzle.
   - LÆ°u toÃ n bá»™ sequence state â†’ action (optimal moves) lÃ m label.
3. **One-hot + features:**
   - Má»—i state mÃ£ hÃ³a one-hot cho má»—i tile (kÃªnh) + kÃªnh Ä‘áº·c trÆ°ng (empty pos, parity).
4. **Data augmentation (deterministic):**
   - 8 biáº¿n thá»ƒ: rotations (90/180/270) + reflections (H/V) â€” Ä‘á»“ng bá»™ vá»›i path (transform action).
5. **Training paradigm:**
   - **Supervised learning**: há»c policy head (predict next optimal move) + value head (Æ°á»›c lÆ°á»£ng steps-to-solve / solvability score).
   - KhÃ´ng dÃ¹ng RL á»Ÿ phase nÃ y â€” má»¥c tiÃªu teach model basic local patterns & moves.

### ğŸ”¹ Cáº¥u hÃ¬nh chi tiáº¿t (full)
```yaml
dataset:
  n_samples: 50000
  move_range: [5, 20]
  augmentation: 8x (rot90, rot180, rot270, flipH, flipV, combos)

model_architecture:
  input_shape: [3, 3, 4]          # one-hot + features
  encoder:
    - Dense: 256, activation: ReLU, dropout: 0.0
    - Dense: 128, activation: ReLU, dropout: 0.0
    - Dense: 64, activation: ReLU, dropout: 0.0
  heads:
    policy_head:
      - Dense: 64, activation: ReLU, dropout: 0.1
      - Dense: 4, activation: Softmax   # up/down/left/right
    value_head:
      - Dense: 64, activation: ReLU
      - Dense: 1, activation: Tanh
    difficulty_head:
      - Dense: 32, activation: ReLU
      - Dense: 4, activation: None  # 4 difficulty classes

training:
  epochs: 200
  batch_size: 128
  optimizer:
    type: Adam
    lr: 0.001
    weight_decay: 0.0
  losses:
    policy_loss: CrossEntropyLoss
    value_loss: MSELoss
    difficulty_loss: CrossEntropyLoss
    lambda_policy: 1.0
    lambda_value: 0.5
    lambda_difficulty: 0.2
  lr_schedule: ReduceLROnPlateau (patience: 10, factor: 0.5, mode: min)
  gradient_clipping: max_norm: 1.0
  seed: 42

checkpointing:
  save_interval_epochs: 10
  save_best_by: train_accuracy
  keep_best: true
  keep_final: false  # foundation.pth bá»‹ xÃ³a sau cleanup
```

### ğŸ”¹ Output

```
phase1_output_3x3.zip/phase1_output/
â”œâ”€â”€ dataset_info.json
â”œâ”€â”€ models/
â”‚   â””â”€â”€ numpuz_3x3_best.pth          # â­ QUAN TRá»ŒNG - dÃ¹ng cho Phase 2
â”œâ”€â”€ README.md
â”œâ”€â”€ training_history_3x3.json
â”œâ”€â”€ train_config_3x3.yaml
â”œâ”€â”€ model_config_3x3.json
â”œâ”€â”€ training_metrics.json
â”œâ”€â”€ training_curves_3x3.png
â””â”€â”€ logs/
    â””â”€â”€ training_phase1.log
```

**âš ï¸ LÆ°u Ã½ Cleanup:**
- File `numpuz_3x3_foundation.pth` bá»‹ xÃ³a sau cleanup (chá»‰ giá»¯ `best.pth`)
- CÃ¡c file timestamped duplicates bá»‹ xÃ³a
- Intermediate epoch checkpoints bá»‹ xÃ³a
- Chá»‰ giá»¯ 1 log file má»›i nháº¥t

---

## ğŸš€ PHRASE 2: SCALING (4x4) â€” `phrase2_4x4.py`

### ğŸ”¹ Input

```
ğŸ“¥ phase1_output/ (hoáº·c auto-search trong nhiá»u vá»‹ trÃ­)
â”œâ”€ numpuz_3x3_best.pth          # â­ Báº®T BUá»˜C
â”œâ”€ model_config_3x3.json         # âš ï¸ TÃ¹y chá»n (cÃ³ thÃ¬ tá»‘t)
â””â”€ train_config_3x3.yaml         # âš ï¸ TÃ¹y chá»n (cÃ³ thÃ¬ tá»‘t)
```

**Auto-Search Locations (theo priority):**
1. Config-specified path
2. `/content/` (Colab working directory)
3. `/content/drive/MyDrive/` (Google Drive)
4. Current directory vÃ  subdirectories
5. Recursive search trong toÃ n bá»™ `/content/`

### ğŸ”¹ Thuáº­t toÃ¡n (Dataset generation, Heuristic & Transfer)

1. **Dataset 4Ã—4 (100k samples):**
   - Sinh puzzle báº±ng random moves tá»« solved state
   - Äá»™ khÃ³: `10â€“50` moves (curriculum stages)
   - Äáº£m báº£o solvable qua inversion count check
   - Data augmentation: 8x (rotation + flip) â†’ 800k total samples

2. **Heuristic nÃ¢ng cao:**
   - Sá»­ dá»¥ng **IDA\*** (Iterative Deepening A*) Ä‘á»ƒ tÃ¬m optimal path
   - Enhanced heuristic = **max(Manhattan distance, Manhattan + Linear Conflict)**
   - Pattern Database (PDB) partition 6-6-3 (precomputed patterns)
   - Node limit: 50,000 nodes per puzzle Ä‘á»ƒ trÃ¡nh timeout

3. **Transfer learning:**
   - **Auto-detect** Phase 1 model trong nhiá»u vá»‹ trÃ­
   - Khá»Ÿi táº¡o model 4Ã—4 by **mapping encoder weights** tá»« `numpuz_3x3_best.pth`
   - Partial transfer cho layers cÃ³ size khÃ¡c biá»‡t (min_rows, min_cols)
   - **Freeze** cÃ¡c encoder layers Ä‘áº§u tiÃªn (`encoder.0`, `encoder.1`) trong giai Ä‘oáº¡n Ä‘áº§u
   - Fine-tune pháº§n policy/value/curriculum heads vÃ  cÃ¡c layer má»Ÿ rá»™ng
   - Progressive unfreezing khi vÃ o curriculum stage "hard"

4. **Curriculum learning:**
   - **Stage 1 (easy):** 10â€“25 moves, 100 epochs
   - **Stage 2 (medium):** 25â€“40 moves, 100 epochs
   - **Stage 3 (hard):** 40â€“50 moves, 100 epochs
   - Auto-advance dá»±a trÃªn epoch progress
   - Unfreeze all layers khi vÃ o hard stage

### ğŸ”¹ Cáº¥u hÃ¬nh chi tiáº¿t
```yaml
dataset:
  n_samples: 100000
  move_range: [10, 50]
  partition_pdb: [6, 6, 3]
  augmentation:
    enabled: true
    methods: [rot90, rot180, rot270, flip_h, flip_v]
    multiplier: 8x  # 100k â†’ 800k samples
  ida_star:
    max_nodes: 50000
    heuristic: max(manhattan, manhattan + linear_conflict)

model_architecture:
  base: transfer_from_3x3
  input_size: 320  # 16 tiles Ã— (17 one-hot + 3 features)
  input_norm: BatchNorm1d
  encoder_layers:
    - Linear: 512, activation: ReLU, dropout: 0.1
    - Linear: 256, activation: ReLU, dropout: 0.1
    - Linear: 128, activation: ReLU, dropout: 0.1
  heads:
    policy_head:
      layers: [128, 4]
      activation: [ReLU, Softmax]
      dropout: 0.1
    value_head:
      layers: [128, 1]
      activation: [ReLU, Tanh]
    curriculum_head:
      layers: [64, 3]
      activation: [ReLU, None]
      note: "3 outputs for easy/medium/hard classification"

transfer_learning:
  source_model: numpuz_3x3_best.pth
  auto_search: true  # Tá»± Ä‘á»™ng tÃ¬m kiáº¿m trong nhiá»u vá»‹ trÃ­
  search_locations:
    - /content/numpuz_3x3_best.pth
    - /content/models/numpuz_3x3_best.pth
    - /content/phase1_output/models/numpuz_3x3_best.pth
    - /content/drive/MyDrive/numpuz_3x3_best.pth
    - (recursive search in /content/)
  strategy: partial_weight_mapping
  freeze:
    - layer_names: ["encoder.0", "encoder.1"]
    - frozen_lr_multiplier: 0.1  # 10% of base LR
  unfreeze_trigger:
    - condition: curriculum_stage == "hard"
    - method: unfreeze_all()

training:
  epochs: 300
  batch_size: 256
  optimizer:
    type: Adam
    lr: 0.0005
    frozen_lr: 0.00005  # For frozen layers
    weight_decay: 1e-4
  losses:
    policy_loss: CrossEntropyLoss
    value_loss: MSELoss
    curriculum_loss: CrossEntropyLoss
    weights:
      lambda_policy: 1.0
      lambda_value: 0.5
      lambda_curriculum: 0.3
  lr_schedule:
    type: CosineAnnealingWarmRestarts
    T_0: 100  # Restart every 100 epochs
    T_mult: 1
    eta_min: 1e-7
  gradient_clipping:
    max_norm: 1.0
  dataloader:
    num_workers: 4
    pin_memory: true
    prefetch_factor: 2
    persistent_workers: true
  seed: 42

curriculum:
  start_stage: easy
  stages:
    easy:
      epochs: 100
      move_range: [10, 25]
      difficulty_class: 0
    medium:
      epochs: 100
      move_range: [25, 40]
      difficulty_class: 1
    hard:
      epochs: 100
      move_range: [40, 50]
      difficulty_class: 2
  progression:
    method: auto_advance
    based_on: epoch_count

evaluation:
  val_split: 0.0  # No validation split (use full dataset for training)
  metrics:
    - policy_accuracy
    - value_mae
    - curriculum_accuracy
    - loss_components

checkpointing:
  save_interval_epochs: 5
  save_best_by: train_accuracy
  keep_checkpoints:
    - best: numpuz_4x4_best.pth
    - phase2: numpuz_4x4_phase2.pth (bá»‹ xÃ³a sau cleanup)
    - periodic: numpuz_4x4_epoch_X.pth (bá»‹ xÃ³a sau cleanup)

cleanup:
  after_training: true
  remove:
    - timestamped_duplicates
    - intermediate_checkpoints (epoch_*.pth)
    - numpuz_4x4_phase2.pth
    - old_logs (giá»¯ 1 log má»›i nháº¥t)
  keep:
    - best_model (numpuz_4x4_best.pth)
    - training_history
    - training_curves
    - configs
    - final_zip_archive
```

### ğŸ”¹ Output

```
phase2_output_4x4.zip/phase2_output/
â”œâ”€â”€ curriculum_progress_4x4.json
â”œâ”€â”€ dataset_info.json
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training_phase2.log
â”œâ”€â”€ model_config_4x4.json
â”œâ”€â”€ models/
â”‚   â””â”€â”€ numpuz_4x4_best.pth          # â­ QUAN TRá»ŒNG - dÃ¹ng cho Phase 3
â”œâ”€â”€ README.md
â”œâ”€â”€ train_config_4x4.yaml
â”œâ”€â”€ training_curves_4x4.png
â”œâ”€â”€ training_history_4x4.json
â””â”€â”€ training_metrics.json
```

**âš ï¸ LÆ°u Ã½ Cleanup:**
- File `numpuz_4x4_phase2.pth` bá»‹ xÃ³a sau cleanup (chá»‰ giá»¯ `best.pth`)
- CÃ¡c file timestamped duplicates bá»‹ xÃ³a
- Intermediate epoch checkpoints bá»‹ xÃ³a
- Chá»‰ giá»¯ 1 log file má»›i nháº¥t

---