# ğŸ¯ FLOW TRAINING - NUMPUZ AI (3x3 â†’ 15x15)

## ğŸ“Œ Tá»•ng quan kiáº¿n trÃºc

Má»—i Ä‘á»™ khÃ³ game NxN Ä‘Æ°á»£c xá»­ lÃ½ trong má»™t **phrase** riÃªng.  
Má»—i phrase lÃ  file Python Ä‘á»™c láº­p `phrase<number>_nxn.py` â€” tá»± Ä‘á»™ng táº¡o dataset, huáº¥n luyá»‡n, xuáº¥t artifacts.  

**Luá»“ng I/O giá»¯a cÃ¡c phrase:**
- Phrase nháº­n model (weights + config) tá»« phrase trÆ°á»›c Ä‘á»ƒ transfer learning.
- Phrase sinh ra: dataset, model, metrics, logs, vÃ  áº£nh trá»±c quan.
- Phrase tiáº¿p theo dÃ¹ng model Ä‘áº§u ra lÃ m input.

---

## ğŸ® PHRASE 1: FOUNDATION (3x3) â€” `phrase1_3x3.py`

### ğŸ”¹ Input
- KhÃ´ng cÃ³ input tá»« phrase trÆ°á»›c (train tá»« Ä‘áº§u).

### ğŸ”¹ Thuáº­t toÃ¡n (Dataset generation & Supervised training)
1. **Sinh puzzle (50k):**
   - Báº¯t Ä‘áº§u tá»« tráº¡ng thÃ¡i solved.
   - Random shuffle báº±ng chuá»—i há»£p lá»‡ vá»›i sá»‘ bÆ°á»›c `k âˆˆ [5,20]`.
   - Kiá»ƒm tra solvability báº±ng inversion count.
2. **Ground-truth path:**
   - DÃ¹ng **A\*** vá»›i heuristic **Manhattan distance** Ä‘á»ƒ tÃ¬m Ä‘Æ°á»ng giáº£i tá»‘i Æ°u cho má»—i puzzle.
   - LÆ°u toÃ n bá»™ sequence state â†’ action (optimal moves) lÃ m label.
3. **One-hot + features:**
   - Má»—i state mÃ£ hÃ³a one-hot cho má»—i tile (kÃªnh) + kÃªnh Ä‘áº·c trÆ°ng (empty pos, parity).
4. **Data augmentation (deterministic):**
   - 8 biáº¿n thá»ƒ: rotations (90/180/270) + reflections (H/V) â€” Ä‘á»“ng bá»™ vá»›i path (transform action).
5. **Training paradigm:**
   - **Supervised learning**: há»c policy head (predict next optimal move) + value head (Æ°á»›c lÆ°á»£ng steps-to-solve / solvability score).
   - KhÃ´ng dÃ¹ng RL á»Ÿ phase nÃ y â€” má»¥c tiÃªu teach model basic local patterns & moves.

### ğŸ”¹ Cáº¥u hÃ¬nh chi tiáº¿t (full)
```yaml
dataset:
  n_samples: 50000
  move_range: [5, 20]
  augmentation: 8x (rot90, rot180, rot270, flipH, flipV, combos)

model_architecture:
  input_shape: [3, 3, 4]          # one-hot + features
  encoder:
    - Dense: 256, activation: ReLU, dropout: 0.0
    - Dense: 128, activation: ReLU, dropout: 0.0
    - Dense: 64, activation: ReLU, dropout: 0.0
  heads:
    policy_head:
      - Dense: 64, activation: ReLU, dropout: 0.1
      - Dense: 4, activation: Softmax   # up/down/left/right
    value_head:
      - Dense: 64, activation: ReLU
      - Dense: 1, activation: Tanh
    difficulty_head:
      - Dense: 32, activation: ReLU
      - Dense: 4, activation: None  # 4 difficulty classes

training:
  epochs: 200
  batch_size: 128
  optimizer:
    type: Adam
    lr: 0.001
    weight_decay: 0.0
  losses:
    policy_loss: CrossEntropyLoss
    value_loss: MSELoss
    difficulty_loss: CrossEntropyLoss
    lambda_policy: 1.0
    lambda_value: 0.5
    lambda_difficulty: 0.2
  lr_schedule: ReduceLROnPlateau (patience: 10, factor: 0.5, mode: min)
  gradient_clipping: max_norm: 1.0
  seed: 42

checkpointing:
  save_interval_epochs: 10
  save_best_by: train_accuracy
  keep_best: true
  keep_final: false  # foundation.pth bá»‹ xÃ³a sau cleanup
```

### ğŸ”¹ Output

```
phase1_output_3x3.zip/phase1_output/
â”œâ”€â”€ dataset_info.json
â”œâ”€â”€ models/
â”‚   â””â”€â”€ numpuz_3x3_best.pth          # â­ QUAN TRá»ŒNG - dÃ¹ng cho Phase 2
â”œâ”€â”€ README.md
â”œâ”€â”€ training_history_3x3.json
â”œâ”€â”€ train_config_3x3.yaml
â”œâ”€â”€ model_config_3x3.json
â”œâ”€â”€ training_metrics.json
â”œâ”€â”€ training_curves_3x3.png
â””â”€â”€ logs/
    â””â”€â”€ training_phase1.log
```

**âš ï¸ LÆ°u Ã½ Cleanup:**
- File `numpuz_3x3_foundation.pth` bá»‹ xÃ³a sau cleanup (chá»‰ giá»¯ `best.pth`)
- CÃ¡c file timestamped duplicates bá»‹ xÃ³a
- Intermediate epoch checkpoints bá»‹ xÃ³a
- Chá»‰ giá»¯ 1 log file má»›i nháº¥t

---

## ğŸš€ PHRASE 2: SCALING (4x4) â€” `phrase2_4x4.py`

### ğŸ”¹ Input

```
ğŸ“¥ phase1_output/ (hoáº·c auto-search trong nhiá»u vá»‹ trÃ­)
â”œâ”€ numpuz_3x3_best.pth          # â­ Báº®T BUá»˜C
â”œâ”€ model_config_3x3.json         # âš ï¸ TÃ¹y chá»n (cÃ³ thÃ¬ tá»‘t)
â””â”€ train_config_3x3.yaml         # âš ï¸ TÃ¹y chá»n (cÃ³ thÃ¬ tá»‘t)
```

**Auto-Search Locations (theo priority):**
1. Config-specified path
2. `/content/` (Colab working directory)
3. `/content/drive/MyDrive/` (Google Drive)
4. Current directory vÃ  subdirectories
5. Recursive search trong toÃ n bá»™ `/content/`

### ğŸ”¹ Thuáº­t toÃ¡n (Dataset generation, Heuristic & Transfer)

1. **Dataset 4Ã—4 (100k samples):**
   - Sinh puzzle báº±ng random moves tá»« solved state
   - Äá»™ khÃ³: `10â€“50` moves (curriculum stages)
   - Äáº£m báº£o solvable qua inversion count check
   - Data augmentation: 8x (rotation + flip) â†’ 800k total samples

2. **Heuristic nÃ¢ng cao:**
   - Sá»­ dá»¥ng **IDA\*** (Iterative Deepening A*) Ä‘á»ƒ tÃ¬m optimal path
   - Enhanced heuristic = **max(Manhattan distance, Manhattan + Linear Conflict)**
   - Pattern Database (PDB) partition 6-6-3 (precomputed patterns)
   - Node limit: 50,000 nodes per puzzle Ä‘á»ƒ trÃ¡nh timeout

3. **Transfer learning:**
   - **Auto-detect** Phase 1 model trong nhiá»u vá»‹ trÃ­
   - Khá»Ÿi táº¡o model 4Ã—4 by **mapping encoder weights** tá»« `numpuz_3x3_best.pth`
   - Partial transfer cho layers cÃ³ size khÃ¡c biá»‡t (min_rows, min_cols)
   - **Freeze** cÃ¡c encoder layers Ä‘áº§u tiÃªn (`encoder.0`, `encoder.1`) trong giai Ä‘oáº¡n Ä‘áº§u
   - Fine-tune pháº§n policy/value/curriculum heads vÃ  cÃ¡c layer má»Ÿ rá»™ng
   - Progressive unfreezing khi vÃ o curriculum stage "hard"

4. **Curriculum learning:**
   - **Stage 1 (easy):** 10â€“25 moves, 100 epochs
   - **Stage 2 (medium):** 25â€“40 moves, 100 epochs
   - **Stage 3 (hard):** 40â€“50 moves, 100 epochs
   - Auto-advance dá»±a trÃªn epoch progress
   - Unfreeze all layers khi vÃ o hard stage

### ğŸ”¹ Cáº¥u hÃ¬nh chi tiáº¿t
```yaml
dataset:
  n_samples: 100000
  move_range: [10, 50]
  partition_pdb: [6, 6, 3]
  augmentation:
    enabled: true
    methods: [rot90, rot180, rot270, flip_h, flip_v]
    multiplier: 8x  # 100k â†’ 800k samples
  ida_star:
    max_nodes: 50000
    heuristic: max(manhattan, manhattan + linear_conflict)

model_architecture:
  base: transfer_from_3x3
  input_size: 320  # 16 tiles Ã— (17 one-hot + 3 features)
  input_norm: BatchNorm1d
  encoder_layers:
    - Linear: 512, activation: ReLU, dropout: 0.1
    - Linear: 256, activation: ReLU, dropout: 0.1
    - Linear: 128, activation: ReLU, dropout: 0.1
  heads:
    policy_head:
      layers: [128, 4]
      activation: [ReLU, Softmax]
      dropout: 0.1
    value_head:
      layers: [128, 1]
      activation: [ReLU, Tanh]
    curriculum_head:
      layers: [64, 3]
      activation: [ReLU, None]
      note: "3 outputs for easy/medium/hard classification"

transfer_learning:
  source_model: numpuz_3x3_best.pth
  auto_search: true  # Tá»± Ä‘á»™ng tÃ¬m kiáº¿m trong nhiá»u vá»‹ trÃ­
  search_locations:
    - /content/numpuz_3x3_best.pth
    - /content/models/numpuz_3x3_best.pth
    - /content/phase1_output/models/numpuz_3x3_best.pth
    - /content/drive/MyDrive/numpuz_3x3_best.pth
    - (recursive search in /content/)
  strategy: partial_weight_mapping
  freeze:
    - layer_names: ["encoder.0", "encoder.1"]
    - frozen_lr_multiplier: 0.1  # 10% of base LR
  unfreeze_trigger:
    - condition: curriculum_stage == "hard"
    - method: unfreeze_all()

training:
  epochs: 300
  batch_size: 256
  optimizer:
    type: Adam
    lr: 0.0005
    frozen_lr: 0.00005  # For frozen layers
    weight_decay: 1e-4
  losses:
    policy_loss: CrossEntropyLoss
    value_loss: MSELoss
    curriculum_loss: CrossEntropyLoss
    weights:
      lambda_policy: 1.0
      lambda_value: 0.5
      lambda_curriculum: 0.3
  lr_schedule:
    type: CosineAnnealingWarmRestarts
    T_0: 100  # Restart every 100 epochs
    T_mult: 1
    eta_min: 1e-7
  gradient_clipping:
    max_norm: 1.0
  dataloader:
    num_workers: 4
    pin_memory: true
    prefetch_factor: 2
    persistent_workers: true
  seed: 42

curriculum:
  start_stage: easy
  stages:
    easy:
      epochs: 100
      move_range: [10, 25]
      difficulty_class: 0
    medium:
      epochs: 100
      move_range: [25, 40]
      difficulty_class: 1
    hard:
      epochs: 100
      move_range: [40, 50]
      difficulty_class: 2
  progression:
    method: auto_advance
    based_on: epoch_count

evaluation:
  val_split: 0.0  # No validation split (use full dataset for training)
  metrics:
    - policy_accuracy
    - value_mae
    - curriculum_accuracy
    - loss_components

checkpointing:
  save_interval_epochs: 5
  save_best_by: train_accuracy
  keep_checkpoints:
    - best: numpuz_4x4_best.pth
    - phase2: numpuz_4x4_phase2.pth (bá»‹ xÃ³a sau cleanup)
    - periodic: numpuz_4x4_epoch_X.pth (bá»‹ xÃ³a sau cleanup)

cleanup:
  after_training: true
  remove:
    - timestamped_duplicates
    - intermediate_checkpoints (epoch_*.pth)
    - numpuz_4x4_phase2.pth
    - old_logs (giá»¯ 1 log má»›i nháº¥t)
  keep:
    - best_model (numpuz_4x4_best.pth)
    - training_history
    - training_curves
    - configs
    - final_zip_archive
```

### ğŸ”¹ Output

```
phase2_output_4x4.zip/phase2_output/
â”œâ”€â”€ curriculum_progress_4x4.json
â”œâ”€â”€ dataset_info.json
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training_phase2.log
â”œâ”€â”€ model_config_4x4.json
â”œâ”€â”€ models/
â”‚   â””â”€â”€ numpuz_4x4_best.pth          # â­ QUAN TRá»ŒNG - dÃ¹ng cho Phase 3
â”œâ”€â”€ README.md
â”œâ”€â”€ train_config_4x4.yaml
â”œâ”€â”€ training_curves_4x4.png
â”œâ”€â”€ training_history_4x4.json
â””â”€â”€ training_metrics.json
```

**âš ï¸ LÆ°u Ã½ Cleanup:**
- File `numpuz_4x4_phase2.pth` bá»‹ xÃ³a sau cleanup (chá»‰ giá»¯ `best.pth`)
- CÃ¡c file timestamped duplicates bá»‹ xÃ³a
- Intermediate epoch checkpoints bá»‹ xÃ³a
- Chá»‰ giá»¯ 1 log file má»›i nháº¥t

---

## ğŸ§  PHRASE 3: MASTERY (5x5) â€” `phrase3_5x5.py`

### ğŸ”¹ Input

```
ğŸ“¥ phase2_output/ (hoáº·c auto-search trong nhiá»u vá»‹ trÃ­)
â”œâ”€ numpuz_4x4_best.pth          # â­ Báº®T BUá»˜C
â”œâ”€ model_config_4x4.json         # âš ï¸ TÃ¹y chá»n (cÃ³ thÃ¬ tá»‘t)
â””â”€ train_config_4x4.yaml         # âš ï¸ TÃ¹y chá»n (cÃ³ thÃ¬ tá»‘t)
```

**Auto-Search Locations (theo priority):**
1. Config-specified path
2. `/content/` (Colab working directory)
3. `/content/drive/MyDrive/` (Google Drive)
4. Current directory vÃ  subdirectories
5. Recursive search trong toÃ n bá»™ `/content/`

### ğŸ”¹ Thuáº­t toÃ¡n (Dataset generation, Heuristic & Transfer)

1. **Dataset 5Ã—5 (200k samples):**
   - Sinh puzzle báº±ng random moves tá»« solved state
   - Äá»™ khÃ³: `15â€“80` moves (curriculum stages)
   - Äáº£m báº£o solvable qua inversion count check
   - Data augmentation: 8x (rotation + flip) â†’ 1.6M total samples

2. **Heuristic nÃ¢ng cao:**
   - Sá»­ dá»¥ng **IDA\*** (Iterative Deepening A*) Ä‘á»ƒ tÃ¬m optimal path
   - Enhanced heuristic = **max(Manhattan distance, Manhattan + Linear Conflict)**
   - Pattern Database (PDB) partition 7-7-6-5 (precomputed patterns)
   - Node limit: 100,000 nodes per puzzle Ä‘á»ƒ trÃ¡nh timeout

3. **Transfer learning:**
   - **Auto-detect** Phase 2 model trong nhiá»u vá»‹ trÃ­
   - Khá»Ÿi táº¡o model 5Ã—5 by **mapping encoder weights** tá»« `numpuz_4x4_best.pth`
   - Partial transfer cho layers cÃ³ size khÃ¡c biá»‡t (min_rows, min_cols)
   - **Freeze** cÃ¡c encoder layers Ä‘áº§u tiÃªn (`encoder.0`, `encoder.1`) trong giai Ä‘oáº¡n Ä‘áº§u
   - Fine-tune pháº§n policy/value/curriculum heads vÃ  cÃ¡c layer má»Ÿ rá»™ng
   - Progressive unfreezing khi vÃ o curriculum stage "hard" vÃ  "expert"

4. **Curriculum learning:**
   - **Stage 1 (easy):** 15â€“35 moves, 100 epochs
   - **Stage 2 (medium):** 35â€“55 moves, 100 epochs
   - **Stage 3 (hard):** 55â€“70 moves, 100 epochs
   - **Stage 4 (expert):** 70â€“80 moves, 100 epochs
   - Auto-advance dá»±a trÃªn epoch progress
   - Unfreeze all layers khi vÃ o hard stage

### ğŸ”¹ Cáº¥u hÃ¬nh chi tiáº¿t
```yaml
dataset:
  n_samples: 200000
  move_range: [15, 80]
  partition_pdb: [7, 7, 6, 5]
  augmentation:
    enabled: true
    methods: [rot90, rot180, rot270, flip_h, flip_v]
    multiplier: 8x  # 200k â†’ 1.6M samples
  ida_star:
    max_nodes: 100000
    heuristic: max(manhattan, manhattan + linear_conflict)

model_architecture:
  base: transfer_from_4x4
  input_size: 725  # 25 tiles Ã— (26 one-hot + 3 features)
  input_norm: BatchNorm1d
  encoder_layers:
    - Linear: 1024, activation: ReLU, dropout: 0.1
    - Linear: 512, activation: ReLU, dropout: 0.1
    - Linear: 256, activation: ReLU, dropout: 0.1
    - Linear: 128, activation: ReLU, dropout: 0.1
  heads:
    policy_head:
      layers: [256, 4]
      activation: [ReLU, Softmax]
      dropout: 0.1
    value_head:
      layers: [256, 1]
      activation: [ReLU, Tanh]
    curriculum_head:
      layers: [128, 4]
      activation: [ReLU, None]
      note: "4 outputs for easy/medium/hard/expert classification"

transfer_learning:
  source_model: numpuz_4x4_best.pth
  auto_search: true
  search_locations:
    - /content/numpuz_4x4_best.pth
    - /content/models/numpuz_4x4_best.pth
    - /content/phase2_output/models/numpuz_4x4_best.pth
    - /content/drive/MyDrive/numpuz_4x4_best.pth
    - (recursive search in /content/)
  strategy: partial_weight_mapping
  freeze:
    - layer_names: ["encoder.0", "encoder.1", "encoder.2"]
    - frozen_lr_multiplier: 0.1  # 10% of base LR
  unfreeze_trigger:
    - condition: curriculum_stage == "hard"
    - method: unfreeze_some()  # Unfreeze encoder.2
    - condition: curriculum_stage == "expert"
    - method: unfreeze_all()

training:
  epochs: 400
  batch_size: 512
  optimizer:
    type: Adam
    lr: 0.0003
    frozen_lr: 0.00003  # For frozen layers
    weight_decay: 1e-4
  losses:
    policy_loss: CrossEntropyLoss
    value_loss: MSELoss
    curriculum_loss: CrossEntropyLoss
    weights:
      lambda_policy: 1.0
      lambda_value: 0.5
      lambda_curriculum: 0.3
  lr_schedule:
    type: CosineAnnealingWarmRestarts
    T_0: 100  # Restart every 100 epochs
    T_mult: 1
    eta_min: 1e-7
  gradient_clipping:
    max_norm: 1.0
  dataloader:
    num_workers: 4
    pin_memory: true
    prefetch_factor: 2
    persistent_workers: true
  seed: 42

curriculum:
  start_stage: easy
  stages:
    easy:
      epochs: 100
      move_range: [15, 35]
      difficulty_class: 0
    medium:
      epochs: 100
      move_range: [35, 55]
      difficulty_class: 1
    hard:
      epochs: 100
      move_range: [55, 70]
      difficulty_class: 2
    expert:
      epochs: 100
      move_range: [70, 80]
      difficulty_class: 3
  progression:
    method: auto_advance
    based_on: epoch_count

evaluation:
  val_split: 0.0  # No validation split (use full dataset for training)
  metrics:
    - policy_accuracy
    - value_mae
    - curriculum_accuracy
    - loss_components

checkpointing:
  save_interval_epochs: 10
  save_best_by: train_accuracy
  keep_checkpoints:
    - best: numpuz_5x5_best.pth
    - phase3: numpuz_5x5_phase3.pth (bá»‹ xÃ³a sau cleanup)
    - periodic: numpuz_5x5_epoch_X.pth (bá»‹ xÃ³a sau cleanup)

cleanup:
  after_training: true
  remove:
    - timestamped_duplicates
    - intermediate_checkpoints (epoch_*.pth)
    - numpuz_5x5_phase3.pth
    - old_logs (giá»¯ 1 log má»›i nháº¥t)
  keep:
    - best_model (numpuz_5x5_best.pth)
    - training_history
    - training_curves
    - configs
    - final_zip_archive
```

### ğŸ”¹ Output

```
phase3_output_5x5.zip/phase3_output/
â”œâ”€â”€ curriculum_progress_5x5.json
â”œâ”€â”€ dataset_info.json
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training_phase3.log
â”œâ”€â”€ model_config_5x5.json
â”œâ”€â”€ models/
â”‚   â””â”€â”€ numpuz_5x5_best.pth          # â­ QUAN TRá»ŒNG - dÃ¹ng cho Phase 4 (náº¿u cÃ³)
â”œâ”€â”€ README.md
â”œâ”€â”€ train_config_5x5.yaml
â”œâ”€â”€ training_curves_5x5.png
â”œâ”€â”€ training_history_5x5.json
â””â”€â”€ training_metrics.json
```

**âš ï¸ LÆ°u Ã½ Cleanup:**
- File `numpuz_5x5_phase3.pth` bá»‹ xÃ³a sau cleanup (chá»‰ giá»¯ `best.pth`)
- CÃ¡c file timestamped duplicates bá»‹ xÃ³a
- Intermediate epoch checkpoints bá»‹ xÃ³a
- Chá»‰ giá»¯ 1 log file má»›i nháº¥t

---

## ğŸ¯ PHASE 4: EXPANSION (6x6) â€” `phase4_6x6.py`

### ğŸ”¹ Input

```
ğŸ“¥ phase3_output/ (hoáº·c auto-search trong nhiá»u vá»‹ trÃ­)
â”œâ”€ numpuz_5x5_best.pth          # â­ Báº®T BUá»˜C
â”œâ”€ model_config_5x5.json         # âš ï¸ TÃ¹y chá»n (cÃ³ thÃ¬ tá»‘t)
â””â”€ train_config_5x5.yaml         # âš ï¸ TÃ¹y chá»n (cÃ³ thÃ¬ tá»‘t)
```

**Auto-Search Locations (theo priority):**
1. Config-specified path
2. `/content/` (Colab working directory)
3. `/content/drive/MyDrive/` (Google Drive)
4. Current directory vÃ  subdirectories
5. Recursive search trong toÃ n bá»™ `/content/`

### ğŸ”¹ Thuáº­t toÃ¡n (Enhanced for 6x6 Complexity)

1. **Dataset 6Ã—6 (300k samples):**
   - Sinh puzzle báº±ng random moves tá»« solved state
   - Äá»™ khÃ³: `20â€“100` moves (curriculum stages)
   - Äáº£m báº£o solvable qua inversion count check
   - Data augmentation: 8x (rotation + flip) â†’ 2.4M total samples

2. **Enhanced heuristics for 6x6:**
   - **Hierarchical IDA***: Break puzzle into subproblems
   - **Enhanced Pattern Database**: 8-8-7-6-5 partition (FLOW.md compliant)
   - **Multi-phase solving**: Solve corners first, then edges
   - **Approximate solving**: Accept suboptimal solutions for very hard puzzles
   - Node limit: 200,000 nodes per puzzle

3. **Advanced transfer learning:**
   - **Hierarchical transfer**: Map different encoder layers for different puzzle regions
   - **Progressive model scaling**: Gradually increase model capacity
   - **Adaptive freezing**: More sophisticated layer freezing strategy

4. **Enhanced curriculum learning:**
   - **5 stages** to handle increased complexity
   - **Adaptive move ranges** based on model performance
   - **Dynamic difficulty adjustment**

### ğŸ”¹ Cáº¥u hÃ¬nh chi tiáº¿t
```yaml
dataset:
  n_samples: 300000
  move_range: [20, 100]
  partition_pdb: [8, 8, 7, 6, 5]  # Enhanced for 6x6
  augmentation:
    enabled: true
    methods: [rot90, rot180, rot270, flip_h, flip_v]
    multiplier: 8x  # 300k â†’ 2.4M samples
  ida_star:
    max_nodes: 200000
    heuristic: hierarchical_manhattan_linear_conflict
    fallback_to_suboptimal: true  # Accept suboptimal for very hard puzzles

model_architecture:
  base: transfer_from_5x5
  input_size: 1584  # 36 tiles Ã— (37 one-hot + 7 features) - enhanced features
  input_norm: GroupNorm  # More stable for large inputs
  encoder_layers:
    - Linear: 2048, activation: ReLU, dropout: 0.2
    - Linear: 1024, activation: ReLU, dropout: 0.2
    - Linear: 512, activation: ReLU, dropout: 0.15
    - Linear: 256, activation: ReLU, dropout: 0.15
    - Linear: 128, activation: ReLU, dropout: 0.1
  heads:
    policy_head:
      layers: [512, 256, 4]  # Deeper for complex decisions
      activation: [ReLU, ReLU, Softmax]
      dropout: 0.15
    value_head:
      layers: [512, 256, 1]
      activation: [ReLU, ReLU, Tanh]
    curriculum_head:
      layers: [256, 128, 5]  # 5 stages for 6x6
      activation: [ReLU, ReLU, None]

transfer_learning:
  source_model: numpuz_5x5_best.pth
  auto_search: true
  strategy: hierarchical_weight_mapping
  freeze:
    - layer_names: ["encoder.0", "encoder.1", "encoder.2"]
    - frozen_lr_multiplier: 0.05  # Even lower LR for frozen layers
  unfreeze_schedule:
    - condition: curriculum_stage == "hard"
      method: unfreeze_layers(["encoder.3"])
    - condition: curriculum_stage == "expert"
      method: unfreeze_layers(["encoder.4"])
    - condition: curriculum_stage == "master"
      method: unfreeze_all()

training:
  epochs: 500  # More epochs for complex puzzles
  batch_size: 512
  gradient_accumulation_steps: 2  # Handle larger models
  optimizer:
    type: AdamW  # Better generalization
    lr: 0.0002
    frozen_lr: 0.00001
    weight_decay: 1e-4
  losses:
    policy_loss: CrossEntropyLoss
    value_loss: SmoothL1Loss  # More robust for value prediction
    curriculum_loss: CrossEntropyLoss
    weights:
      lambda_policy: 1.0
      lambda_value: 0.7  # Higher weight for value prediction
      lambda_curriculum: 0.4
  lr_schedule:
    type: CosineAnnealingWarmRestarts
    T_0: 100
    T_mult: 1
    eta_min: 1e-7
  gradient_clipping:
    max_norm: 0.5  # Tighter clipping for stability

curriculum:
  start_stage: easy
  stages:
    easy:
      epochs: 100
      move_range: [20, 40]
      difficulty_class: 0
    medium:
      epochs: 100
      move_range: [40, 60]
      difficulty_class: 1
    hard:
      epochs: 100
      move_range: [60, 75]
      difficulty_class: 2
    expert:
      epochs: 100
      move_range: [75, 85]
      difficulty_class: 3
    master:
      epochs: 100
      move_range: [85, 100]
      difficulty_class: 4
  progression:
    method: performance_based  # Advance based on accuracy thresholds
    accuracy_threshold: 0.85

evaluation:
  val_split: 0.1  # Use validation for early stopping
  early_stopping_patience: 20
  metrics:
    - policy_accuracy
    - value_mae
    - curriculum_accuracy
    - loss_components

checkpointing:
  save_interval_epochs: 10
  save_best_by: val_accuracy
  keep_checkpoints:
    - best: numpuz_6x6_best.pth
    - phase4: numpuz_6x6_phase4.pth (bá»‹ xÃ³a sau cleanup)
    - periodic: numpuz_6x6_epoch_X.pth (bá»‹ xÃ³a sau cleanup)

cleanup:
  after_training: true
  remove:
    - timestamped_duplicates
    - intermediate_checkpoints (epoch_*.pth)
    - numpuz_6x6_phase4.pth
    - old_logs (giá»¯ 1 log má»›i nháº¥t)
  keep:
    - best_model (numpuz_6x6_best.pth)
    - training_history
    - training_curves
    - configs
    - final_zip_archive
```

### ğŸ”¹ Output

```
phase4_output_6x6.zip/phase4_output/
â”œâ”€â”€ curriculum_progress_6x6.json
â”œâ”€â”€ dataset_info.json
â”œâ”€â”€ enhanced_heuristics_report.json
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ training_phase4.log
â”œâ”€â”€ model_config_6x6.json
â”œâ”€â”€ models/
â”‚   â””â”€â”€ numpuz_6x6_best.pth          # â­ QUAN TRá»ŒNG - dÃ¹ng cho Phase 5
â”œâ”€â”€ README.md
â”œâ”€â”€ train_config_6x6.yaml
â”œâ”€â”€ training_curves_6x6.png
â”œâ”€â”€ training_history_6x6.json
â””â”€â”€ training_metrics.json
```

**âš ï¸ LÆ°u Ã½ Cleanup:**
- File `numpuz_6x6_phase4.pth` bá»‹ xÃ³a sau cleanup (chá»‰ giá»¯ `best.pth`)
- CÃ¡c file timestamped duplicates bá»‹ xÃ³a
- Intermediate epoch checkpoints bá»‹ xÃ³a
- Chá»‰ giá»¯ 1 log file má»›i nháº¥t

---